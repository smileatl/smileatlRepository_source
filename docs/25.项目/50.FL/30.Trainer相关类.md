---
title: Trainer相关类
date: 2023-08-11 11:33:32
permalink: /pages/b6f5ee/
categories:
  - 项目
  - FL
tags:
  - 
author: 
  name: smileatl
  link: https://github.com/smileatl
---
# Trainer相关类

## Trainer抽象基类

抽象基类是一种特殊的类，用于定义接口和公共方法，并强制子类实现这些方法。

此处，`Trainer` 类是一个抽象基类，定义了一些方法，但没有提供具体的实现。这些方法包括：

- `train_with_grad(self, grad)`：使用梯度进行训练。
- `compute_gradient(self, model, data, y)`：计算模型在给定数据和标签上的梯度。
- `aggregate(self, models)`：聚合多个模型。
- `evaluate(self, data, label)`：评估模型在给定数据和标签上的性能。
- `get_weights(self)`：获取模型的权重。
- `set_weights(self)`：设置模型的权重。
- `predict(self)`：对新数据进行预测。

通过使用 `abc` 模块中的 `abstractmethod` 装饰器，这些方法被标记为抽象方法，意味着它们必须在子类中进行具体的实现。子类必须提供这些方法的实现，否则将引发 `TypeError`。

通过定义抽象基类，可以确保子类具有一致的接口和行为，从而遵循相同的约定和规范。此外，抽象基类还可以作为文档化工具，提供关于接口和方法的说明。

需要注意的是，抽象基类本身不能被实例化。它只能用作其他类的基类，以确保子类按照指定的接口进行实现。

如果要使用该抽象基类，您需要创建一个具体的子类，并为抽象方法提供具体的实现。这样，您就可以实例化子类并使用其提供的方法了。

```python
import abc

class Trainer():
    def __init__(self) -> None:
        pass

    @abc.abstractmethod
    def train_with_grad(self, grad):
        pass

    @abc.abstractmethod
    def compute_gradient(self, model, data, y):
        pass

    @abc.abstractmethod
    def aggregate(self, models):
        pass

    @abc.abstractmethod
    def evaluate(self, data, label):
        pass

    @abc.abstractmethod
    def get_weights(self):
        pass

    @abc.abstractmethod
    def set_weights(self):
        pass

    @abc.abstractmethod
    def predict(self):
        pass
```



## TFTrainer训练模型类

继承自抽象基类，需要进行方法的具体实现

```cpp
import numpy as np
from Trainer import Trainer
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error


# 定义训练的模型，还有其他需要用到的函数
class TFTrainer(Trainer):
    def __init__(self) -> None:
        import tensorflow as tf
        super().__init__()

        self.__optimizer = tf.keras.optimizers.Adam()
        self.__loss = tf.keras.losses.MeanAbsoluteError()

        self.__model = tf.keras.Sequential([
            # tf.keras.layers.Flatten(input_shape=(28, 28)),
            tf.keras.layers.GRU(128, input_shape=(None, 1)),
            tf.keras.layers.Dense(32, activation='relu'),
            tf.keras.layers.Dense(1)

        self.__model.compile(
            optimizer=self.__optimizer,
            loss=self.__loss,
            metrics=['mape']
        )

    def train_with_grad(self, grad):
        self.__optimizer.apply_gradients(zip(grad, self.__model.trainable_variables))

    def compute_gradient(self, model, data, y):
        import tensorflow as tf
        self.__model.set_weights(model)
        with tf.GradientTape() as tape:
            y_pred = self.__model(data)
            loss = self.__loss(y, y_pred)
        grads = tape.gradient(loss, self.__model.trainable_variables)
        return [item.numpy() for item in grads]

    def aggregate(self, models):
        self.__model.set_weights(list(np.mean(list(models.values()), axis=0)))

    def evaluate(self, data, label, scaler_minmax):
        pred = self.__model.predict(data)
        loss, acc = self.__model.evaluate(data, label)

        # 因为前面缩放到了0-1之间，现在利对象里的inverse_transform函数反归一化到1-7之间
        label = scaler_minmax.inverse_transform(label)
        pred = scaler_minmax.inverse_transform(pred)

        mseerror = mean_squared_error(label, pred)
        maeerror = mean_absolute_error(label, pred)
        mapeerror = mean_absolute_percentage_error(label, pred)*100
        return loss, acc, mseerror, maeerror, mapeerror, pred

        # return self.__model.evaluate(data, label)

    def get_weights(self):
        return self.__model.get_weights()

    def set_weights(self, model):
        return self.__model.set_weights(model)

    def predict(self, data):
        return self.__model.predict(data)

if __name__ == '__main__':
    r = TFTrainer()
```

